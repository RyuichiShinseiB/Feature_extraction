{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"EUU-McVcFGJ0"},"source":["# 第9回講義 宿題\n","\n","## 課題\n","自己教師あり学習を用いて事前学習を行い，得られた表現をLinear probingで評価してみましょう．  \n","ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．   \n","\n","## 目標精度\n","なし\n","- 自己教師あり学習の手法によっては計算リソースによって性能が大きく変わるため，目標精度は設定しておりません．\n","- ただし以下の工夫を行うことで計算リソースが少なくとも，長い学習を分割して行うことができます．  \n","    - model，optimizer, schedulerを一定エポックで保存して，読み込むことで学習を再開することができます．\n","    - 演習のようにschedulerを実装した場合は，保存は必要なく同じ引数でインスタンスを作成して\\_\\_call\\_\\_の際に与えるepochを学習の続きから与えれば動作します．  \n","    - 参考: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","\n","## ルール\n","- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください．\n","- 自己教師あり学習では以下のセルで指定されている`x_train`以外の学習データは用いないでください．\n","- Linear probingの際には`x_train`, `t_train`以外の学習データは用いないでください．\n","\n","## 提出方法\n","- 2つのファイルを提出していただきます．\n","    - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください．\n","    - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)．\n","\n","- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n","\n","- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n","\n","## 評価方法\n","\n","- 予測ラベルの`t_test`に対するAccuracyで評価します．\n","- 即時採点しLeader Boardを更新します．\n","- 締切時の点数を最終的な評価とします．"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OtPxAOyKHN_l"},"source":["### ドライブのマウント"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29845,"status":"ok","timestamp":1688559363987,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"UQM_SpiDHfso","outputId":"d9462a9b-9611-482d-8b74-1111e5873350"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yN8Jq5a8J5HV"},"source":["### データの読み込み\n","- この部分は修正しないでください．"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11283,"status":"ok","timestamp":1688559375267,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"LNpUF5xOJ8bG"},"outputs":[],"source":["import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm_notebook as tqdm\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","#学習データ\n","x_train = np.load('/workdir/tmp/data/x_train.npy')\n","t_train = np.load('/workdir/tmp/data/t_train.npy')\n","\n","#テストデータ\n","x_test = np.load('/workdir/tmp/data/x_test.npy')\n","\n","class train_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_train, t_train):\n","        data = x_train.astype('float32')\n","        self.x_train = []\n","        for i in range(data.shape[0]):\n","            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n","        self.t_train = t_train\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_train)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n","\n","class test_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_test):\n","        data = x_test.astype('float32')\n","        self.x_test = []\n","        for i in range(data.shape[0]):\n","            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_test)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_test[idx])\n","\n","trainval_data = train_dataset(x_train, t_train)\n","test_data = test_dataset(x_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fSqA6Ni3MDSX"},"source":["### データローダの準備  "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1688559375774,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"63ODMwChMEy_"},"outputs":[],"source":["val_size = 3000\n","train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),])# WRITE ME\n","valid_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","test_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","\n","batch_size = 128\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    valid_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_test = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MQpTXlwbKRdW"},"source":["### 自己教師あり学習の実装\n","- 初期の形式はMAEを利用することを想定していますが，他の自己教師あり学習を利用していただいて構いません．   "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5356,"status":"ok","timestamp":1688559385106,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"TzlJ4q1uKagF"},"outputs":[],"source":["import math\n","from torch import nn, optim\n","import torch.nn.functional as nnf\n","from einops.layers.torch import Rearrange\n","from einops import rearrange\n","\n","def fix_seed(seed=1234):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","fix_seed(seed=42)\n","\n","\n","def random_indexes(size):\n","    \"\"\"\n","    パッチをランダムに並べ替えるためのindexを生成する関数．\n","\n","    Argument\n","    --------\n","    size : int\n","        入力されるパッチの数（系列長Nと同じ値）．\n","    \"\"\"\n","    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n","    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n","    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n","\n","    return forward_indexes, backward_indexes\n","\n","\n","def take_indexes(sequences, indexes):\n","    \"\"\"\n","    パッチを並べ替えるための関数．\n","\n","    Argument\n","    --------\n","    sequences : torch.Tensor\n","        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n","    indexes : np.ndarray\n","        並べ替えるために利用するindex．\n","        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n","    \"\"\"\n","    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads, dim_head, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        dim : int\n","            入力データの次元数．埋め込み次元数と一致する．\n","        heads : int\n","            ヘッドの数．\n","        dim_head : int\n","            各ヘッドのデータの次元数．\n","        dropout : float\n","            Dropoutの確率(default=0.)．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.dim = dim\n","        self.dim_head = dim_head\n","        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n","        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n","\n","        self.heads = heads\n","        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n","\n","        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Q, K, Vに変換するための全結合層\n","        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n","        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n","        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n","\n","        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n","        self.to_out = nn.Sequential(\n","            nn.Linear(in_features=inner_dim, out_features=dim),\n","            nn.Dropout(dropout),\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        B: バッチサイズ\n","        N: 系列長\n","        D: データの次元数(dim)\n","        \"\"\"\n","        B, N, D = x.size()\n","\n","        # 入力データをQ, K, Vに変換する\n","        # (B, N, dim) -> (B, N, inner_dim)\n","        q = self.to_q(x)\n","        k = self.to_k(x)\n","        v = self.to_v(x)\n","\n","        # Q, K, Vをヘッドに分割する\n","        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n","        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","\n","        # QK^T / sqrt(d_k)を計算する\n","        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n","        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","\n","        # ソフトマックス関数でスコアを算出し，Dropoutをする\n","        attn = self.attend(dots)\n","        attn = self.dropout(attn)\n","\n","        # softmax(QK^T / sqrt(d_k))Vを計算する\n","        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n","        out = torch.matmul(attn ,v)\n","\n","        # もとの形に戻す\n","        # (B, heads, N, dim_head) -> (B, N, dim)\n","        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n","\n","        # 次元が違っていればもとに戻して出力\n","        # 表現の可視化のためにattention mapも返すようにしておく\n","        return self.to_out(out), attn\n","\n","\n","class FFN(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        dim : int\n","            入力データの次元数．\n","        hidden_dim : int\n","            隠れ層の次元．\n","        dropout : float\n","            各全結合層の後のDropoutの確率(default=0.)．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(in_features=dim, out_features=hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(in_features=hidden_dim, out_features=dim),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        (B, D) -> (B, D)\n","        B: バッチサイズ\n","        D: 次元数\n","        \"\"\"\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n","        \"\"\"\n","        TransformerのEncoder Blockの実装．\n","\n","        Arguments\n","        ---------\n","        dim : int\n","            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        dropout : float\n","            Droptou層の確率p．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n","        self.attn = Attention(dim, heads, dim_head, dropout)\n","        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n","        self.ffn = FFN(dim, mlp_dim, dropout)\n","\n","    def forward(self, x, return_attn=False):\n","        \"\"\"\n","        x: (B, N, dim)\n","        B: バッチサイズ\n","        N: 系列長\n","        dim: 埋め込み次元\n","        \"\"\"\n","        y, attn = self.attn(self.attn_ln(x))\n","        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n","            return attn\n","        x = y + x\n","        out = self.ffn(self.ffn_ln(x)) + x\n","\n","        return out\n","\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n","        \"\"\"\n","        入力画像をパッチごとに埋め込むための層．\n","\n","        Arguments\n","        ---------\n","        image_size : Tuple[int]\n","            入力画像のサイズ．\n","        patch_size : Tuple[int]\n","            各パッチのサイズ．\n","        in_channels : int\n","            入力画像のチャネル数．\n","        embed_dim : int\n","            埋め込み後の次元数．\n","        \"\"\"\n","        super().__init__()\n","\n","        image_height, image_width = image_size\n","        patch_height, patch_width = patch_size\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n","        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n","            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        B: バッチサイズ\n","        C: 入力画像のチャネル数\n","        H: 入力画像の高さ\n","        W: 入力画像の幅\n","        \"\"\"\n","        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)\n","\n","\n","class PatchShuffle(nn.Module):\n","    def __init__(self, ratio):\n","        # ratio: Encoderに入力しないパッチの割合\n","        super().__init__()\n","        self.ratio = ratio\n","\n","    def forward(self, patches):\n","        \"\"\"\n","        B: バッチサイズ\n","        N: 系列長（＝パッチの数）\n","        dim: 次元数（＝埋め込みの次元数）\n","        \"\"\"\n","        B, N, dim = patches.shape\n","        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n","\n","        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n","        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n","        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n","\n","        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n","        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n","\n","        return patches, forward_indexes, backward_indexes\n","\n","\n","class MAE_Encoder(torch.nn.Module):\n","    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n","                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        num_layer : int\n","            Encoderに含まれるBlockの数．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        mask_ratio : float\n","            入力パッチのマスクする割合．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","        img_height, img_width = image_size\n","        patch_height, patch_width = patch_size\n","        num_patches = (img_height // patch_height) * (img_width // patch_width)\n","\n","        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n","        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n","        self.shuffle = PatchShuffle(mask_ratio)\n","\n","        # 入力画像をパッチに分割する\n","        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n","\n","        # Encoder（Blockを重ねる）\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n","\n","        self.layer_norm = nn.LayerNorm(emb_dim)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        torch.nn.init.normal_(self.cls_token, std=0.02)\n","        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n","\n","    def forward(self, img):\n","        # 1. 入力画像をパッチに分割して，positional embeddingする\n","        patches = self.patchify(img)\n","        patches = patches + self.pos_embedding\n","\n","        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n","        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n","\n","        # class tokenを結合\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n","\n","        # 3. Encoderで入力データを処理する\n","        features = self.layer_norm(self.transformer(patches))\n","\n","        return features, backward_indexes\n","\n","\n","class MAE_Decoder(nn.Module):\n","    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=4,\n","                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        num_layer : int\n","            Decoderに含まれるBlockの数．\n","        heads : int\n","            Multi-Head Attentionのヘッドの数．\n","        dim_head : int\n","            Multi-Head Attentionの各ヘッドの次元数．\n","        mlp_dim : int\n","            Feed-Forward Networkの隠れ層の次元数．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","        img_height, img_width = image_size\n","        patch_height, patch_width = patch_size\n","        num_patches = (img_height // patch_height) * (img_width // patch_width)\n","\n","        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n","        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n","\n","        # Decoder(Blockを重ねる）\n","        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n","\n","        # 埋め込みされた表現から画像を復元するためのhead\n","        self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n","        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n","        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        torch.nn.init.normal_(self.mask_token, std=0.02)\n","        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n","\n","    def forward(self, features, backward_indexes):\n","        # 系列長\n","        T = features.shape[1]\n","\n","        # class tokenがある分backward_indexesの最初に0を追加する\n","        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n","        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n","\n","        # 1. mask_tokenを結合して並べ替える．\n","        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n","        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n","        features = take_indexes(features, backward_indexes)\n","        features = features + self.pos_embedding\n","\n","        features = self.transformer(features)\n","\n","        # class tokenを除去する\n","        # (B, N+1, dim) -> (B, N, dim)\n","        features = features[:, 1:, :]\n","\n","        # 2. 画像を再構成する．\n","        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n","        patches = self.head(features)\n","\n","        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n","        mask = torch.zeros_like(patches)\n","        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n","        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n","\n","        img = self.patch2img(patches)\n","        mask = self.patch2img(mask)\n","\n","        return img, mask\n","\n","\n","class MAE_ViT(torch.nn.Module):\n","    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192,\n","                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n","                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n","                 mask_ratio=0.75, dropout=0.):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        image_size : List[int]\n","            入力画像の大きさ．\n","        patch_size : List[int]\n","            各パッチの大きさ．\n","        emb_dim : int\n","            データを埋め込む次元の数．\n","        {enc/dec}_layers : int\n","            Encoder / Decoderに含まれるBlockの数．\n","        {enc/dec}_heads : int\n","            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n","        {enc/dec}_dim_head : int\n","            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n","        {enc/dec}_mlp_dim : int\n","            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n","        mask_ratio : float\n","            入力パッチのマスクする割合．\n","        dropout : float\n","            ドロップアウトの確率．\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n","                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n","        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n","                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n","\n","    def forward(self, img):\n","        features, backward_indexes = self.encoder(img)\n","        rec_img, mask = self.decoder(features, backward_indexes)\n","        return rec_img, mask\n","\n","    def get_last_selfattention(self, x):\n","        patches = self.encoder.patchify(x)\n","        patches = patches + self.encoder.pos_embedding\n","\n","        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        for i, block in enumerate(self.encoder.transformer):\n","            if i < len(self.encoder.transformer) - 1:\n","                patches = block(patches)\n","            else:\n","                return block(patches, return_attn=True)\n","\n","\n","# cosine scheduler\n","class CosineScheduler:\n","    def __init__(self, epochs, lr, warmup_length=5):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epochs : int\n","            学習のエポック数．\n","        lr : float\n","            学習率．\n","        warmup_length : int\n","            warmupを適用するエポック数．\n","        \"\"\"\n","        self.epochs = epochs\n","        self.lr = lr\n","        self.warmup = warmup_length\n","\n","    def __call__(self, epoch):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epoch : int\n","            現在のエポック数．\n","        \"\"\"\n","        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n","        progress = np.clip(progress, 0.0, 1.0)\n","        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n","\n","        if self.warmup:\n","            lr = lr * min(1., (epoch+1) / self.warmup)\n","\n","        return lr\n","\n","def set_lr(lr, optimizer):\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","# ハイパーパラメータの設定\n","config = {\n","    \"image_size\": [32, 32],\n","    \"patch_size\": [2, 2],\n","    \"emb_dim\": 192,\n","    \"enc_layers\": 12,\n","    \"enc_heads\": 3,\n","    \"enc_dim_head\": 64,\n","    \"enc_mlp_dim\": 192,\n","    \"dec_layers\": 4,\n","    \"dec_heads\": 3,\n","    \"dec_dim_head\": 64,\n","    \"dec_mlp_dim\": 192,\n","    \"mask_ratio\": 0.75,\n","    \"dropout\": 0.\n","}\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = MAE_ViT(**config).to(device)\n","\n","epochs = 100\n","lr = 0.003\n","warmup_length = epochs // 10\n","batch_size = 512\n","step_count = 0\n","optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n","scheduler = CosineScheduler(epochs, lr, warmup_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1688540880175,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"etMNIZhZ3zal","outputId":"5508994c-bf6d-4470-e4d4-11cbd3b5c619"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["print(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uR8uNlkCxo3d"},"source":["### 事前学習（自己教師あり学習）"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10106797,"status":"ok","timestamp":1688556255439,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"TfIeOGbbxqXx","outputId":"a3cf1ee3-515e-4edb-87fb-558c98294ebb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch[1 / 100] Train Loss: 0.0774 Valid Loss: 0.0485\n","Epoch[2 / 100] Train Loss: 0.0584 Valid Loss: 0.0477\n","Epoch[3 / 100] Train Loss: 0.0510 Valid Loss: 0.0446\n","Epoch[4 / 100] Train Loss: 0.0509 Valid Loss: 0.0454\n","Epoch[5 / 100] Train Loss: 0.0453 Valid Loss: 0.0408\n","Epoch[6 / 100] Train Loss: 0.0424 Valid Loss: 0.0367\n","Epoch[7 / 100] Train Loss: 0.0397 Valid Loss: 0.0345\n","Epoch[8 / 100] Train Loss: 0.0357 Valid Loss: 0.0323\n","Epoch[9 / 100] Train Loss: 0.0374 Valid Loss: 0.0334\n","Epoch[10 / 100] Train Loss: 0.0322 Valid Loss: 0.0309\n","Epoch[11 / 100] Train Loss: 0.0313 Valid Loss: 0.0315\n","Epoch[12 / 100] Train Loss: 0.0301 Valid Loss: 0.0285\n","Epoch[13 / 100] Train Loss: 0.0275 Valid Loss: 0.0257\n","Epoch[14 / 100] Train Loss: 0.0256 Valid Loss: 0.0232\n","Epoch[15 / 100] Train Loss: 0.0229 Valid Loss: 0.0218\n","Epoch[16 / 100] Train Loss: 0.0215 Valid Loss: 0.0204\n","Epoch[17 / 100] Train Loss: 0.0199 Valid Loss: 0.0185\n","Epoch[18 / 100] Train Loss: 0.0187 Valid Loss: 0.0179\n","Epoch[19 / 100] Train Loss: 0.0177 Valid Loss: 0.0169\n","Epoch[20 / 100] Train Loss: 0.0171 Valid Loss: 0.0164\n","Epoch[21 / 100] Train Loss: 0.0168 Valid Loss: 0.0162\n","Epoch[22 / 100] Train Loss: 0.0162 Valid Loss: 0.0162\n","Epoch[23 / 100] Train Loss: 0.0160 Valid Loss: 0.0157\n","Epoch[24 / 100] Train Loss: 0.0153 Valid Loss: 0.0149\n","Epoch[25 / 100] Train Loss: 0.0149 Valid Loss: 0.0161\n","Epoch[26 / 100] Train Loss: 0.0147 Valid Loss: 0.0140\n","Epoch[27 / 100] Train Loss: 0.0145 Valid Loss: 0.0142\n","Epoch[28 / 100] Train Loss: 0.0141 Valid Loss: 0.0137\n","Epoch[29 / 100] Train Loss: 0.0139 Valid Loss: 0.0136\n","Epoch[30 / 100] Train Loss: 0.0136 Valid Loss: 0.0134\n","Epoch[31 / 100] Train Loss: 0.0134 Valid Loss: 0.0131\n","Epoch[32 / 100] Train Loss: 0.0133 Valid Loss: 0.0130\n","Epoch[33 / 100] Train Loss: 0.0131 Valid Loss: 0.0132\n","Epoch[34 / 100] Train Loss: 0.0128 Valid Loss: 0.0128\n","Epoch[35 / 100] Train Loss: 0.0127 Valid Loss: 0.0126\n","Epoch[36 / 100] Train Loss: 0.0125 Valid Loss: 0.0125\n","Epoch[37 / 100] Train Loss: 0.0124 Valid Loss: 0.0124\n","Epoch[38 / 100] Train Loss: 0.0123 Valid Loss: 0.0120\n","Epoch[39 / 100] Train Loss: 0.0122 Valid Loss: 0.0119\n","Epoch[40 / 100] Train Loss: 0.0120 Valid Loss: 0.0119\n","Epoch[41 / 100] Train Loss: 0.0119 Valid Loss: 0.0119\n","Epoch[42 / 100] Train Loss: 0.0118 Valid Loss: 0.0117\n","Epoch[43 / 100] Train Loss: 0.0118 Valid Loss: 0.0118\n","Epoch[44 / 100] Train Loss: 0.0117 Valid Loss: 0.0117\n","Epoch[45 / 100] Train Loss: 0.0116 Valid Loss: 0.0116\n","Epoch[46 / 100] Train Loss: 0.0115 Valid Loss: 0.0113\n","Epoch[47 / 100] Train Loss: 0.0115 Valid Loss: 0.0113\n","Epoch[48 / 100] Train Loss: 0.0114 Valid Loss: 0.0113\n","Epoch[49 / 100] Train Loss: 0.0114 Valid Loss: 0.0113\n","Epoch[50 / 100] Train Loss: 0.0113 Valid Loss: 0.0111\n","Epoch[51 / 100] Train Loss: 0.0112 Valid Loss: 0.0111\n","Epoch[52 / 100] Train Loss: 0.0112 Valid Loss: 0.0109\n","Epoch[53 / 100] Train Loss: 0.0111 Valid Loss: 0.0111\n","Epoch[54 / 100] Train Loss: 0.0110 Valid Loss: 0.0110\n","Epoch[55 / 100] Train Loss: 0.0110 Valid Loss: 0.0109\n","Epoch[56 / 100] Train Loss: 0.0109 Valid Loss: 0.0109\n","Epoch[57 / 100] Train Loss: 0.0109 Valid Loss: 0.0109\n","Epoch[58 / 100] Train Loss: 0.0108 Valid Loss: 0.0108\n","Epoch[59 / 100] Train Loss: 0.0108 Valid Loss: 0.0107\n","Epoch[60 / 100] Train Loss: 0.0107 Valid Loss: 0.0107\n","Epoch[61 / 100] Train Loss: 0.0107 Valid Loss: 0.0106\n","Epoch[62 / 100] Train Loss: 0.0107 Valid Loss: 0.0107\n","Epoch[63 / 100] Train Loss: 0.0106 Valid Loss: 0.0106\n","Epoch[64 / 100] Train Loss: 0.0106 Valid Loss: 0.0106\n","Epoch[65 / 100] Train Loss: 0.0105 Valid Loss: 0.0105\n","Epoch[66 / 100] Train Loss: 0.0105 Valid Loss: 0.0105\n","Epoch[67 / 100] Train Loss: 0.0105 Valid Loss: 0.0104\n","Epoch[68 / 100] Train Loss: 0.0104 Valid Loss: 0.0103\n","Epoch[69 / 100] Train Loss: 0.0104 Valid Loss: 0.0104\n","Epoch[70 / 100] Train Loss: 0.0104 Valid Loss: 0.0103\n","Epoch[71 / 100] Train Loss: 0.0103 Valid Loss: 0.0102\n","Epoch[72 / 100] Train Loss: 0.0103 Valid Loss: 0.0103\n","Epoch[73 / 100] Train Loss: 0.0102 Valid Loss: 0.0103\n","Epoch[74 / 100] Train Loss: 0.0102 Valid Loss: 0.0102\n","Epoch[75 / 100] Train Loss: 0.0102 Valid Loss: 0.0102\n","Epoch[76 / 100] Train Loss: 0.0102 Valid Loss: 0.0102\n","Epoch[77 / 100] Train Loss: 0.0101 Valid Loss: 0.0102\n","Epoch[78 / 100] Train Loss: 0.0101 Valid Loss: 0.0101\n","Epoch[79 / 100] Train Loss: 0.0101 Valid Loss: 0.0100\n","Epoch[80 / 100] Train Loss: 0.0101 Valid Loss: 0.0101\n","Epoch[81 / 100] Train Loss: 0.0100 Valid Loss: 0.0100\n","Epoch[82 / 100] Train Loss: 0.0100 Valid Loss: 0.0099\n","Epoch[83 / 100] Train Loss: 0.0100 Valid Loss: 0.0100\n","Epoch[84 / 100] Train Loss: 0.0100 Valid Loss: 0.0100\n","Epoch[85 / 100] Train Loss: 0.0100 Valid Loss: 0.0100\n","Epoch[86 / 100] Train Loss: 0.0099 Valid Loss: 0.0100\n","Epoch[87 / 100] Train Loss: 0.0099 Valid Loss: 0.0099\n","Epoch[88 / 100] Train Loss: 0.0099 Valid Loss: 0.0099\n","Epoch[89 / 100] Train Loss: 0.0099 Valid Loss: 0.0099\n","Epoch[90 / 100] Train Loss: 0.0099 Valid Loss: 0.0099\n","Epoch[91 / 100] Train Loss: 0.0099 Valid Loss: 0.0098\n","Epoch[92 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[93 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[94 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[95 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[96 / 100] Train Loss: 0.0098 Valid Loss: 0.0097\n","Epoch[97 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[98 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[99 / 100] Train Loss: 0.0098 Valid Loss: 0.0099\n","Epoch[100 / 100] Train Loss: 0.0098 Valid Loss: 0.0098\n"]}],"source":["for epoch in range(epochs):\n","    # スケジューラで学習率を更新する\n","    new_lr = scheduler(epoch)\n","    set_lr(new_lr, optimizer)\n","\n","    total_train_loss = 0.\n","    total_valid_loss = 0.\n","\n","    # モデルの訓練\n","    for x, _ in dataloader_train:\n","        step_count += 1\n","        model.train()\n","        x = x.to(device)\n","\n","        rec_img, mask = model(x)\n","        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n","        train_loss.backward()\n","\n","        if step_count % 8 == 0:  # 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        total_train_loss += train_loss.item()\n","\n","    # モデルの評価\n","    with torch.no_grad():\n","        for x, _ in dataloader_valid:\n","            model.eval()\n","\n","            with torch.no_grad():\n","                x = x.to(device)\n","\n","                rec_img, mask = model(x)\n","                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n","\n","                total_valid_loss += valid_loss.item()\n","\n","\n","    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(dataloader_train):.4f} Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\")\n","\n","# モデルを保存しておく\n","torch.save(model.state_dict(), \"tmp/MYmodel/MAE_pretrain_params.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHOBi4auxuPR"},"source":["### Linear probing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1688559503295,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"1GeuhPBryfQa"},"outputs":[],"source":["val_size = 3000\n","train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),])# WRITE ME\n","valid_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","test_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","\n","batch_size = 128\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    valid_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_test = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1312,"status":"ok","timestamp":1688559495741,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"_ycgOJ3g-FC3","outputId":"0560003d-ca67-468d-8033-853a20da0c40"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(\"/workdir/tmp/MYmodel/MAE_pretrain_params.pth\"))"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":347,"status":"ok","timestamp":1688559508943,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"z8n5wVT-xvv1"},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self, encoder: MAE_Encoder, num_classes=10):\n","        super().__init__()\n","        self.cls_token = encoder.cls_token\n","        self.pos_embedding = encoder.pos_embedding\n","        self.patchify = encoder.patchify\n","        self.transformer = encoder.transformer\n","        self.layer_norm = encoder.layer_norm\n","        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n","\n","    def forward(self, img):\n","        patches = self.patchify(img)\n","        patches = patches + self.pos_embedding  # positional embedding\n","\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        features = self.layer_norm(self.transformer(patches))\n","        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n","        return logits\n","\n","    def get_last_selfattention(self, x):\n","        patches = self.patchify(x)\n","        patches = patches + self.pos_embedding\n","\n","        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n","        for i, block in enumerate(self.transformer):\n","            if i < len(self.transformer) - 1:\n","                patches = block(patches)\n","            else:\n","                return block(patches, return_attn=True)\n","\n","class MLPClassifier(nn.Module):\n","    def __init__(self, hidden_dims: tuple[int, int] = (64, 32), num_classes: int = 10, dropout_ratio: float = 0.3) -> None:\n","        super(MLPClassifier, self).__init__()\n","        self.layer1 = nn.Linear(config[\"emb_dim\"], hidden_dims[0])\n","        self.dropout1=nn.Dropout(dropout_ratio)\n","        self.layer2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n","        self.dropout2 = nn.Dropout(dropout_ratio)\n","        self.layer3 = nn.Linear(hidden_dims[1], num_classes)\n","        \n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.layer1(x)\n","        x = self.dropout1(x)\n","        x = nnf.leaky_relu(x)\n","        x = self.layer2(x)\n","        x = self.dropout2(x)\n","        x = nnf.leaky_relu(x)\n","        x = self.layer3(x)\n","        x = nnf.softmax(x, dim=1)\n","        return x\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","encoder = model.encoder# WRITE ME\n","classifier_model = MLPClassifier().to(device) # WRITE ME\n","epochs = 100\n","lr = 0.001\n","warmup_length = 10\n","batch_size = 64\n","optimizer = optim.AdamW(classifier_model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\n","scheduler = CosineScheduler(epochs, lr, warmup_length)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5T6QiHwyTRj","outputId":"66dbb8f3-d883-4ae0-c6a1-f9c0ecb5339e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch[1 / 100] Train Loss: 2.2947 Train Acc.: 0.1436 Valid Loss: 2.2752 Valid Acc.: 0.1989\n","Epoch[2 / 100] Train Loss: 2.2267 Train Acc.: 0.2327 Valid Loss: 2.1939 Valid Acc.: 0.2626\n","Epoch[3 / 100] Train Loss: 2.1749 Train Acc.: 0.2850 Valid Loss: 2.1626 Valid Acc.: 0.2889\n","Epoch[4 / 100] Train Loss: 2.1449 Train Acc.: 0.3154 Valid Loss: 2.1299 Valid Acc.: 0.3337\n","Epoch[5 / 100] Train Loss: 2.1159 Train Acc.: 0.3451 Valid Loss: 2.1020 Valid Acc.: 0.3567\n","Epoch[6 / 100] Train Loss: 2.0932 Train Acc.: 0.3670 Valid Loss: 2.0889 Valid Acc.: 0.3666\n","Epoch[7 / 100] Train Loss: 2.0802 Train Acc.: 0.3782 Valid Loss: 2.0724 Valid Acc.: 0.3889\n","Epoch[8 / 100] Train Loss: 2.0698 Train Acc.: 0.3876 Valid Loss: 2.0732 Valid Acc.: 0.3851\n","Epoch[9 / 100] Train Loss: 2.0637 Train Acc.: 0.3943 Valid Loss: 2.0582 Valid Acc.: 0.4033\n","Epoch[10 / 100] Train Loss: 2.0587 Train Acc.: 0.3975 Valid Loss: 2.0641 Valid Acc.: 0.3941\n","Epoch[11 / 100] Train Loss: 2.0567 Train Acc.: 0.3985 Valid Loss: 2.0636 Valid Acc.: 0.3962\n","Epoch[12 / 100] Train Loss: 2.0526 Train Acc.: 0.4052 Valid Loss: 2.0600 Valid Acc.: 0.3941\n","Epoch[13 / 100] Train Loss: 2.0489 Train Acc.: 0.4082 Valid Loss: 2.0514 Valid Acc.: 0.4043\n","Epoch[14 / 100] Train Loss: 2.0478 Train Acc.: 0.4092 Valid Loss: 2.0500 Valid Acc.: 0.3952\n","Epoch[15 / 100] Train Loss: 2.0436 Train Acc.: 0.4118 Valid Loss: 2.0455 Valid Acc.: 0.4091\n","Epoch[16 / 100] Train Loss: 2.0440 Train Acc.: 0.4121 Valid Loss: 2.0550 Valid Acc.: 0.4032\n","Epoch[17 / 100] Train Loss: 2.0415 Train Acc.: 0.4151 Valid Loss: 2.0479 Valid Acc.: 0.4086\n","Epoch[18 / 100] Train Loss: 2.0399 Train Acc.: 0.4171 Valid Loss: 2.0446 Valid Acc.: 0.4071\n","Epoch[19 / 100] Train Loss: 2.0393 Train Acc.: 0.4166 Valid Loss: 2.0420 Valid Acc.: 0.4130\n","Epoch[20 / 100] Train Loss: 2.0390 Train Acc.: 0.4182 Valid Loss: 2.0416 Valid Acc.: 0.4155\n","Epoch[21 / 100] Train Loss: 2.0385 Train Acc.: 0.4176 Valid Loss: 2.0421 Valid Acc.: 0.4155\n","Epoch[22 / 100] Train Loss: 2.0352 Train Acc.: 0.4206 Valid Loss: 2.0370 Valid Acc.: 0.4198\n","Epoch[23 / 100] Train Loss: 2.0344 Train Acc.: 0.4201 Valid Loss: 2.0359 Valid Acc.: 0.4220\n","Epoch[24 / 100] Train Loss: 2.0329 Train Acc.: 0.4242 Valid Loss: 2.0423 Valid Acc.: 0.4144\n","Epoch[25 / 100] Train Loss: 2.0335 Train Acc.: 0.4231 Valid Loss: 2.0324 Valid Acc.: 0.4293\n","Epoch[26 / 100] Train Loss: 2.0324 Train Acc.: 0.4249 Valid Loss: 2.0366 Valid Acc.: 0.4217\n","Epoch[27 / 100] Train Loss: 2.0327 Train Acc.: 0.4233 Valid Loss: 2.0322 Valid Acc.: 0.4257\n","Epoch[28 / 100] Train Loss: 2.0299 Train Acc.: 0.4265 Valid Loss: 2.0433 Valid Acc.: 0.4120\n","Epoch[29 / 100] Train Loss: 2.0307 Train Acc.: 0.4261 Valid Loss: 2.0320 Valid Acc.: 0.4259\n","Epoch[30 / 100] Train Loss: 2.0281 Train Acc.: 0.4296 Valid Loss: 2.0229 Valid Acc.: 0.4314\n","Epoch[31 / 100] Train Loss: 2.0301 Train Acc.: 0.4275 Valid Loss: 2.0466 Valid Acc.: 0.4062\n","Epoch[32 / 100] Train Loss: 2.0317 Train Acc.: 0.4238 Valid Loss: 2.0342 Valid Acc.: 0.4202\n","Epoch[33 / 100] Train Loss: 2.0264 Train Acc.: 0.4303 Valid Loss: 2.0361 Valid Acc.: 0.4141\n","Epoch[34 / 100] Train Loss: 2.0259 Train Acc.: 0.4310 Valid Loss: 2.0311 Valid Acc.: 0.4252\n","Epoch[35 / 100] Train Loss: 2.0258 Train Acc.: 0.4313 Valid Loss: 2.0315 Valid Acc.: 0.4232\n","Epoch[36 / 100] Train Loss: 2.0256 Train Acc.: 0.4312 Valid Loss: 2.0300 Valid Acc.: 0.4251\n","Epoch[37 / 100] Train Loss: 2.0263 Train Acc.: 0.4288 Valid Loss: 2.0321 Valid Acc.: 0.4276\n","Epoch[38 / 100] Train Loss: 2.0261 Train Acc.: 0.4298 Valid Loss: 2.0379 Valid Acc.: 0.4187\n","Epoch[39 / 100] Train Loss: 2.0238 Train Acc.: 0.4336 Valid Loss: 2.0297 Valid Acc.: 0.4243\n","Epoch[40 / 100] Train Loss: 2.0234 Train Acc.: 0.4318 Valid Loss: 2.0322 Valid Acc.: 0.4195\n","Epoch[41 / 100] Train Loss: 2.0259 Train Acc.: 0.4301 Valid Loss: 2.0224 Valid Acc.: 0.4358\n","Epoch[42 / 100] Train Loss: 2.0232 Train Acc.: 0.4330 Valid Loss: 2.0341 Valid Acc.: 0.4178\n","Epoch[43 / 100] Train Loss: 2.0227 Train Acc.: 0.4326 Valid Loss: 2.0376 Valid Acc.: 0.4193\n","Epoch[44 / 100] Train Loss: 2.0212 Train Acc.: 0.4338 Valid Loss: 2.0292 Valid Acc.: 0.4273\n","Epoch[45 / 100] Train Loss: 2.0202 Train Acc.: 0.4359 Valid Loss: 2.0269 Valid Acc.: 0.4317\n","Epoch[46 / 100] Train Loss: 2.0190 Train Acc.: 0.4402 Valid Loss: 2.0395 Valid Acc.: 0.4177\n","Epoch[47 / 100] Train Loss: 2.0210 Train Acc.: 0.4359 Valid Loss: 2.0341 Valid Acc.: 0.4145\n","Epoch[48 / 100] Train Loss: 2.0216 Train Acc.: 0.4344 Valid Loss: 2.0305 Valid Acc.: 0.4225\n","Epoch[49 / 100] Train Loss: 2.0191 Train Acc.: 0.4383 Valid Loss: 2.0327 Valid Acc.: 0.4269\n","Epoch[50 / 100] Train Loss: 2.0200 Train Acc.: 0.4378 Valid Loss: 2.0375 Valid Acc.: 0.4158\n","Epoch[51 / 100] Train Loss: 2.0171 Train Acc.: 0.4401 Valid Loss: 2.0228 Valid Acc.: 0.4351\n","Epoch[52 / 100] Train Loss: 2.0201 Train Acc.: 0.4372 Valid Loss: 2.0288 Valid Acc.: 0.4243\n","Epoch[53 / 100] Train Loss: 2.0213 Train Acc.: 0.4356 Valid Loss: 2.0186 Valid Acc.: 0.4427\n","Epoch[54 / 100] Train Loss: 2.0210 Train Acc.: 0.4373 Valid Loss: 2.0159 Valid Acc.: 0.4371\n","Epoch[55 / 100] Train Loss: 2.0180 Train Acc.: 0.4389 Valid Loss: 2.0194 Valid Acc.: 0.4412\n","Epoch[56 / 100] Train Loss: 2.0164 Train Acc.: 0.4422 Valid Loss: 2.0256 Valid Acc.: 0.4282\n","Epoch[57 / 100] Train Loss: 2.0166 Train Acc.: 0.4411 Valid Loss: 2.0232 Valid Acc.: 0.4320\n","Epoch[58 / 100] Train Loss: 2.0150 Train Acc.: 0.4431 Valid Loss: 2.0210 Valid Acc.: 0.4362\n","Epoch[59 / 100] Train Loss: 2.0146 Train Acc.: 0.4430 Valid Loss: 2.0270 Valid Acc.: 0.4332\n","Epoch[60 / 100] Train Loss: 2.0155 Train Acc.: 0.4426 Valid Loss: 2.0232 Valid Acc.: 0.4298\n","Epoch[61 / 100] Train Loss: 2.0174 Train Acc.: 0.4406 Valid Loss: 2.0266 Valid Acc.: 0.4271\n","Epoch[62 / 100] Train Loss: 2.0131 Train Acc.: 0.4438 Valid Loss: 2.0275 Valid Acc.: 0.4315\n","Epoch[63 / 100] Train Loss: 2.0141 Train Acc.: 0.4429 Valid Loss: 2.0188 Valid Acc.: 0.4382\n","Epoch[64 / 100] Train Loss: 2.0151 Train Acc.: 0.4426 Valid Loss: 2.0129 Valid Acc.: 0.4430\n","Epoch[65 / 100] Train Loss: 2.0124 Train Acc.: 0.4464 Valid Loss: 2.0266 Valid Acc.: 0.4259\n","Epoch[66 / 100] Train Loss: 2.0140 Train Acc.: 0.4432 Valid Loss: 2.0138 Valid Acc.: 0.4416\n","Epoch[67 / 100] Train Loss: 2.0092 Train Acc.: 0.4487 Valid Loss: 2.0270 Valid Acc.: 0.4265\n","Epoch[68 / 100] Train Loss: 2.0139 Train Acc.: 0.4445 Valid Loss: 2.0210 Valid Acc.: 0.4379\n","Epoch[69 / 100] Train Loss: 2.0126 Train Acc.: 0.4453 Valid Loss: 2.0230 Valid Acc.: 0.4347\n","Epoch[70 / 100] Train Loss: 2.0106 Train Acc.: 0.4466 Valid Loss: 2.0225 Valid Acc.: 0.4372\n","Epoch[71 / 100] Train Loss: 2.0105 Train Acc.: 0.4480 Valid Loss: 2.0216 Valid Acc.: 0.4389\n","Epoch[72 / 100] Train Loss: 2.0105 Train Acc.: 0.4483 Valid Loss: 2.0218 Valid Acc.: 0.4335\n","Epoch[73 / 100] Train Loss: 2.0071 Train Acc.: 0.4520 Valid Loss: 2.0111 Valid Acc.: 0.4451\n","Epoch[74 / 100] Train Loss: 2.0144 Train Acc.: 0.4438 Valid Loss: 2.0217 Valid Acc.: 0.4329\n","Epoch[75 / 100] Train Loss: 2.0078 Train Acc.: 0.4512 Valid Loss: 2.0099 Valid Acc.: 0.4473\n","Epoch[76 / 100] Train Loss: 2.0078 Train Acc.: 0.4512 Valid Loss: 2.0219 Valid Acc.: 0.4302\n","Epoch[77 / 100] Train Loss: 2.0078 Train Acc.: 0.4518 Valid Loss: 2.0235 Valid Acc.: 0.4293\n","Epoch[78 / 100] Train Loss: 2.0085 Train Acc.: 0.4501 Valid Loss: 2.0176 Valid Acc.: 0.4420\n","Epoch[79 / 100] Train Loss: 2.0075 Train Acc.: 0.4523 Valid Loss: 2.0251 Valid Acc.: 0.4248\n","Epoch[80 / 100] Train Loss: 2.0090 Train Acc.: 0.4497 Valid Loss: 2.0202 Valid Acc.: 0.4380\n","Epoch[81 / 100] Train Loss: 2.0087 Train Acc.: 0.4493 Valid Loss: 2.0195 Valid Acc.: 0.4441\n","Epoch[82 / 100] Train Loss: 2.0065 Train Acc.: 0.4509 Valid Loss: 2.0227 Valid Acc.: 0.4314\n","Epoch[83 / 100] Train Loss: 2.0065 Train Acc.: 0.4526 Valid Loss: 2.0142 Valid Acc.: 0.4391\n","Epoch[84 / 100] Train Loss: 2.0063 Train Acc.: 0.4522 Valid Loss: 2.0213 Valid Acc.: 0.4336\n","Epoch[85 / 100] Train Loss: 2.0076 Train Acc.: 0.4517 Valid Loss: 2.0243 Valid Acc.: 0.4284\n","Epoch[86 / 100] Train Loss: 2.0065 Train Acc.: 0.4510 Valid Loss: 2.0114 Valid Acc.: 0.4463\n","Epoch[87 / 100] Train Loss: 2.0087 Train Acc.: 0.4488 Valid Loss: 2.0135 Valid Acc.: 0.4399\n","Epoch[88 / 100] Train Loss: 2.0085 Train Acc.: 0.4491 Valid Loss: 2.0064 Valid Acc.: 0.4495\n","Epoch[89 / 100] Train Loss: 2.0065 Train Acc.: 0.4524 Valid Loss: 2.0121 Valid Acc.: 0.4480\n","Epoch[90 / 100] Train Loss: 2.0072 Train Acc.: 0.4515 Valid Loss: 2.0175 Valid Acc.: 0.4384\n","Epoch[91 / 100] Train Loss: 2.0077 Train Acc.: 0.4503 Valid Loss: 2.0236 Valid Acc.: 0.4288\n","Epoch[92 / 100] Train Loss: 2.0103 Train Acc.: 0.4470 Valid Loss: 2.0072 Valid Acc.: 0.4521\n","Epoch[93 / 100] Train Loss: 2.0064 Train Acc.: 0.4518 Valid Loss: 2.0116 Valid Acc.: 0.4462\n","Epoch[94 / 100] Train Loss: 2.0061 Train Acc.: 0.4532 Valid Loss: 2.0238 Valid Acc.: 0.4320\n","Epoch[95 / 100] Train Loss: 2.0052 Train Acc.: 0.4532 Valid Loss: 2.0167 Valid Acc.: 0.4393\n","Epoch[96 / 100] Train Loss: 2.0074 Train Acc.: 0.4502 Valid Loss: 2.0163 Valid Acc.: 0.4450\n","Epoch[97 / 100] Train Loss: 2.0065 Train Acc.: 0.4505 Valid Loss: 2.0169 Valid Acc.: 0.4346\n","Epoch[98 / 100] Train Loss: 2.0084 Train Acc.: 0.4496 Valid Loss: 2.0277 Valid Acc.: 0.4250\n","Epoch[99 / 100] Train Loss: 2.0068 Train Acc.: 0.4514 Valid Loss: 2.0148 Valid Acc.: 0.4374\n","Epoch[100 / 100] Train Loss: 2.0065 Train Acc.: 0.4517 Valid Loss: 2.0180 Valid Acc.: 0.4425\n"]}],"source":["encoder.eval()\n","for epoch in range(epochs):\n","    new_lr = scheduler(epoch)\n","    set_lr(new_lr, optimizer)\n","\n","    total_train_loss = 0.\n","    total_train_acc = 0.\n","    total_valid_loss = 0.\n","    total_valid_acc = 0.\n","    for x, t in dataloader_train:\n","        x, t = x.to(device), t.to(device)\n","        feature, _ = encoder(x)\n","        pred = classifier_model(feature[:, 0, :])\n","\n","        train_loss = criterion(pred, t)\n","        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n","\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += train_loss.item()\n","        total_train_acc += train_acc\n","\n","    with torch.no_grad():\n","        for x, t in dataloader_valid:\n","            x, t = x.to(device), t.to(device)\n","            feature, _ = encoder(x)\n","            pred = classifier_model(feature[:, 0, :])\n","\n","            valid_loss = criterion(pred, t)\n","            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n","\n","            total_valid_loss += valid_loss.item()\n","            total_valid_acc += valid_acc\n","\n","    print(f\"Epoch[{epoch+1} / {epochs}]\",\n","          f\"Train Loss: {total_train_loss/len(dataloader_train):.4f}\",\n","          f\"Train Acc.: {total_train_acc/len(dataloader_train):.4f}\",\n","          f\"Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\",\n","          f\"Valid Acc.: {total_valid_acc/len(dataloader_valid):.4f}\")\n","\n","torch.save(classifier_model.state_dict(), \"/workdir/tmp/MYmodel/MAE_classifier_params.pth\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([128, 192])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["feature[:,0,:].shape"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"72n19Q_RyqWl"},"outputs":[],"source":["classifier_model.eval()\n","\n","t_pred = []\n","for x in dataloader_test:\n","    x = x.to(device)\n","    feature, _ = encoder(x)\n","    y = classifier_model(feature[:, 0, :])\n","\n","    # モデルの出力を予測値のスカラーに変換\n","    pred = y.argmax(1).tolist()\n","    t_pred.extend(pred)\n","\n","submission = pd.Series(t_pred, name='label')\n","submission.to_csv(\"submission_pred.csv\", header=True, index_label='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwKJ8IBcy5TG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
