{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"EUU-McVcFGJ0"},"source":["# 第9回講義 宿題\n","\n","## 課題\n","自己教師あり学習を用いて事前学習を行い，得られた表現をLinear probingで評価してみましょう．  \n","ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．   \n","\n","## 目標精度\n","なし\n","- 自己教師あり学習の手法によっては計算リソースによって性能が大きく変わるため，目標精度は設定しておりません．\n","- ただし以下の工夫を行うことで計算リソースが少なくとも，長い学習を分割して行うことができます．  \n","    - model，optimizer, schedulerを一定エポックで保存して，読み込むことで学習を再開することができます．\n","    - 演習のようにschedulerを実装した場合は，保存は必要なく同じ引数でインスタンスを作成して\\_\\_call\\_\\_の際に与えるepochを学習の続きから与えれば動作します．  \n","    - 参考: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","\n","## ルール\n","- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください．\n","- 自己教師あり学習では以下のセルで指定されている`x_train`以外の学習データは用いないでください．\n","- Linear probingの際には`x_train`, `t_train`以外の学習データは用いないでください．\n","\n","## 提出方法\n","- 2つのファイルを提出していただきます．\n","    - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください．\n","    - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)．\n","\n","- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n","\n","- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n","\n","## 評価方法\n","\n","- 予測ラベルの`t_test`に対するAccuracyで評価します．\n","- 即時採点しLeader Boardを更新します．\n","- 締切時の点数を最終的な評価とします．"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OtPxAOyKHN_l"},"source":["### ドライブのマウント"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29845,"status":"ok","timestamp":1688559363987,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"UQM_SpiDHfso","outputId":"d9462a9b-9611-482d-8b74-1111e5873350"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yN8Jq5a8J5HV"},"source":["### データの読み込み\n","- この部分は修正しないでください．"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11283,"status":"ok","timestamp":1688559375267,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"LNpUF5xOJ8bG"},"outputs":[],"source":["import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm_notebook as tqdm\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","#学習データ\n","x_train = np.load('/workdir/tmp/data/x_train.npy')\n","t_train = np.load('/workdir/tmp/data/t_train.npy')\n","\n","#テストデータ\n","x_test = np.load('/workdir/tmp/data/x_test.npy')\n","\n","class train_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_train, t_train):\n","        data = x_train.astype('float32')\n","        self.x_train = []\n","        for i in range(data.shape[0]):\n","            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n","        self.t_train = t_train\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_train)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n","\n","class test_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_test):\n","        data = x_test.astype('float32')\n","        self.x_test = []\n","        for i in range(data.shape[0]):\n","            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_test)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_test[idx])\n","\n","trainval_data = train_dataset(x_train, t_train)\n","test_data = test_dataset(x_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fSqA6Ni3MDSX"},"source":["### データローダの準備  "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1688559375774,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"63ODMwChMEy_"},"outputs":[],"source":["val_size = 3000\n","train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),])# WRITE ME\n","valid_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","test_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","\n","batch_size = 128\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    valid_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_test = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MQpTXlwbKRdW"},"source":["### 自己教師あり学習の実装\n","- 初期の形式はMAEを利用することを想定していますが，他の自己教師あり学習を利用していただいて構いません．   "]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5356,"status":"ok","timestamp":1688559385106,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"TzlJ4q1uKagF"},"outputs":[],"source":["import torch.nn.functional as nnf\n","from torch import nn, optim\n","from typing import TypeAlias\n","\n","Tensor: TypeAlias = torch.Tensor\n","\n","\n","def fix_seed(seed=1234):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","fix_seed(seed=42)\n","\n","class MultiHeadsSelfAttention(nn.Module):\n","    def __init__(\n","        self, emb_dim: int = 384, heads: int = 3, dropout: float = 0.1\n","    ) -> None:\n","        \"\"\"Multi-Heads Self-Attention (MHSA) Block\n","\n","        Parameters\n","        ----------\n","        emb_dim : int, optional\n","            Length of embedded vector, by default 384\n","        heads : int, optional\n","            Number of heads, by default 3\n","        dropout : float, optional\n","            dropout rate, by default 0.1\n","        \"\"\"\n","\n","        super(MultiHeadsSelfAttention, self).__init__()\n","        self.heads = heads\n","        self.emb_dim = emb_dim\n","        self.head_dim = emb_dim // heads\n","        self.sqrt_dh = self.head_dim**0.5\n","\n","        #\n","        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False)\n","        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False)\n","        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n","\n","        self.attn_drop = nn.Dropout(dropout)\n","\n","        self.w_o = nn.Sequential(\n","            nn.Linear(emb_dim, emb_dim), nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, z: Tensor) -> tuple[Tensor, Tensor]:\n","        \"\"\"forward propagation\n","\n","        Parameters\n","        ----------\n","        z : Tensor\n","            Input to MHSA.\n","            shape = (B, N, D)\n","            B: Batch size, N: Number of tokens, D: Length of embedded vector\n","\n","        Returns\n","        -------\n","        out: Tensor\n","            Output from MHSA\n","            shape = (B, N, D)\n","        \"\"\"\n","        batch_size, num_patch, _ = z.size()\n","\n","        # (B, N, D) -> (B, N, D)\n","        q = self.w_q(z)\n","        k = self.w_k(z)\n","        v = self.w_v(z)\n","\n","        # (B, N, D) -> (B, N, h, D//h) -> (B, h, N, D//h)\n","        q = q.view(batch_size, num_patch, self.heads, self.head_dim).transpose(\n","            1, 2\n","        )\n","        k = k.view(batch_size, num_patch, self.heads, self.head_dim).transpose(\n","            1, 2\n","        )\n","        v = v.view(batch_size, num_patch, self.heads, self.head_dim).transpose(\n","            1, 2\n","        )\n","\n","        # (B, h, N, D//h) X (B, h, D//h, N) -> (B, h, N, N)\n","        dots = torch.matmul(q, k.transpose(2, 3)) / self.sqrt_dh\n","\n","        attn = nnf.softmax(dots, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        # (B, h, N, N) X (B, h, N, D//h) -> (B, h, N, D//h)\n","        out = torch.matmul(attn, v)\n","        # (B, h, N, D//h) -> (B, N, h, D//h)\n","        out = out.transpose(1, 2)\n","        # (B, N, h, D//h) -> (B, N, D)\n","        out = out.reshape(batch_size, num_patch, self.emb_dim)\n","        # (B, N, D) -> (B, N, D)\n","        out = self.w_o(out)\n","        return out, attn\n","\n","\n","class FFN(nn.Module):\n","    def __init__(\n","        self, emb_dim: int, hidden_dim: int, dropout: float = 0.1\n","    ) -> None:\n","        \"\"\"Feed Forward Network\n","\n","        Parameters\n","        ----------\n","        emb_dim : int\n","            Length of embedded vector\n","        hidden_dim : int\n","            Length of hidden layer\n","        dropout : float, optional\n","            Dropout rate, by default 0.1\n","        \"\"\"\n","        super(FFN, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(emb_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, emb_dim),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"forward propagation\n","\n","        Parameters\n","        ----------\n","        x : Tensor\n","            input from MHSA to FFN.\n","            Shape is (B, N, D)\n","            B: Batch size, N: Number of tokens, D: Length of embedded vector\n","\n","        Returns\n","        -------\n","        out : Tensor\n","            output from FFN.\n","            Shape is (B, N, D)\n","        \"\"\"\n","        out: Tensor = self.net(x)\n","        return out\n","\n","\n","class Block(nn.Module):\n","    def __init__(\n","        self,\n","        emb_dim: int = 384,\n","        heads: int = 8,\n","        hidden_dim: int = 384 * 4,\n","        dropout: float = 0.0,\n","    ) -> None:\n","        \"\"\"basic Transformer forward block\n","\n","        Parameters\n","        ----------\n","        emb_dim : int, optional\n","            Length of embedded vector, by default 384\n","        heads : int, optional\n","            Number of heads, by default 8\n","        hidden_dim : int, optional\n","            Dimension of hidden layer, by default 384*4\n","        dropout : float, optional\n","            Dropout rate, by default 0.0\n","        \"\"\"\n","        super(Block, self).__init__()\n","        self.ln1 = nn.LayerNorm(emb_dim)\n","        self.mhsa = MultiHeadsSelfAttention(emb_dim, heads, dropout)\n","        self.ln2 = nn.LayerNorm(emb_dim)\n","        self.ffn = FFN(emb_dim, hidden_dim, dropout)\n","\n","    def forward(self, z: Tensor) -> Tensor:\n","        \"\"\"Forward propagation\n","\n","        Parameters\n","        ----------\n","        z : Tensor\n","            Input to basic Transformer block. shape = (B, N, D)\n","            B: batch size, N: number of tokens, D: length of embedded vector\n","\n","        Returns\n","        -------\n","        (out, attn) : tuple[Tensor, Tensor]\n","            output to next basic Transformer block or MLP head.\n","            shape = (B, N, D)\n","        \"\"\"\n","        out, _ = self.mhsa(self.ln1(z))\n","        out += z\n","        out = self.ffn(self.ln2(out)) + out\n","        return torch.Tensor(out)\n","\n","    def get_attn(self, z: Tensor) -> Tensor:\n","        \"\"\"Get attention map from Multi-Heads Self-Attention\n","\n","        Parameters\n","        ----------\n","        z : Tensor\n","            Patches. Shape is (B, N, D)\n","\n","        Returns\n","        -------\n","        attn: Tensor\n","            Self attention from MHSA. Shape is (B, N, N)\n","        \"\"\"\n","        _, attn = self.mhsa(self.ln1(z))\n","        return torch.Tensor(attn)\n","\n","\n","class PatchShuffle(nn.Module):\n","    def __init__(self, ratio: float) -> None:\n","        \"\"\"Patches shuffling and some patches masking\n","\n","        Parameters\n","        ----------\n","        ratio : float\n","            Ratio of patches to be masked.\n","        \"\"\"\n","        super().__init__()\n","        self.ratio = ratio\n","\n","    def forward(self, patches: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n","        \"\"\"Perform shuffling and masking of patches\n","\n","        Parameters\n","        ----------\n","        patches : Tensor\n","            Patches of image. Shape is (B, N, D)\n","            B: batch size, N: number of tokens, D: length of embedded vector\n","\n","        Returns\n","        -------\n","        patches : Tensor\n","            Perform shuffling and masking of patches.\n","            Shape is (B, n, D).\n","            B: batch size, n: number of remaining unmasked tokens,\n","            D: length of embedded vector\n","            n = int(N * (1 - mask_ratio))\n","        forward_indexes : Tensor\n","            Indexes for shuffling the original patches.\n","            Shape is (B, N)\n","        backward_indexes : Tensor\n","            Indexes for restoring to the original patches.\n","            Shape is (B, N)\n","        \"\"\"\n","        batch_size, num_patch, _ = patches.shape\n","        remain_patches = int(num_patch * (1 - self.ratio))\n","\n","        indexes: list[tuple[Tensor, Tensor]] = [\n","            self.patch_random_indexes(num_patch) for _ in range(batch_size)\n","        ]\n","        forward_indexes = torch.vstack([idx[0] for idx in indexes]).to(\n","            dtype=torch.long, device=patches.device\n","        )\n","        backward_indexes = torch.vstack([idx[1] for idx in indexes]).to(\n","            dtype=torch.long, device=patches.device\n","        )\n","\n","        patches = self.take_indexes(patches, forward_indexes)\n","        patches = patches[:, :remain_patches, :]\n","        return patches, forward_indexes, backward_indexes\n","\n","    @staticmethod\n","    def patch_random_indexes(num_patch: int) -> tuple[Tensor, Tensor]:\n","        \"\"\"Function to generate indexes for random sorting of patches\n","\n","        Parameters\n","        ----------\n","        num_patch : int\n","            Number of patches N.\n","\n","        Returns\n","        -------\n","        forward_indexes: Tensor\n","            Indexes of randomly arranged patches.\n","            The first element is 0, which is the index of the cls_token\n","            and the elements after it are the indexes of the patches.\n","        backward_indexes: Tensor\n","            Indexes for restoring the indexes.\n","            Same as forward_indexes for the first element.\n","        \"\"\"\n","        _forward_indexes = torch.randperm(num_patch - 1)\n","        forward_indexes = torch.cat([torch.zeros(1), _forward_indexes + 1])\n","        backward_indexes = torch.argsort(forward_indexes)\n","        return forward_indexes, backward_indexes\n","\n","    @staticmethod\n","    def take_indexes(sequences: Tensor, indexes: Tensor) -> Tensor:\n","        \"\"\"Function to sort patches\n","\n","        Parameters\n","        ----------\n","        sequences : Tensor\n","            Data splitted input image to patches.\n","            shape = (B, N, D)\n","        indexes : Tensor\n","            Indexes to sorting data\n","\n","        Returns\n","        -------\n","        _type_\n","            _description_\n","        \"\"\"\n","\n","        return torch.gather(\n","            sequences,\n","            dim=1,\n","            index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]),\n","        )\n","\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(\n","        self,\n","        input_channels: int = 3,\n","        emb_dim: int = 384,\n","        num_patch_row: int = 2,\n","        image_size: int = 32,\n","    ) -> None:\n","        super(PatchEmbedding, self).__init__()\n","        self.input_channels = input_channels\n","        self.emb_dim = emb_dim\n","        self.num_patch_row = num_patch_row\n","        self.image_size = image_size\n","\n","        self.num_patch = self.num_patch_row**2\n","        self.patch_size = int(self.image_size // self.num_patch_row)\n","\n","        self.patch_emb_layer = nn.Conv2d(\n","            in_channels=self.input_channels,\n","            out_channels=self.emb_dim,\n","            kernel_size=self.patch_size,\n","            stride=self.patch_size,\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        # (B, C, H, W) -> (B, D, H/P, W/P)\n","        z_0: Tensor = self.patch_emb_layer(x)\n","\n","        # (B, D, H/P, W/P) -> (B, D, H*W/P^2) -> (B, H*W/P^2, D)\n","        z_0 = z_0.flatten(2).transpose(1, 2)\n","\n","        return z_0\n","\n","\n","class MAEencoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_channels: int = 3,\n","        emb_dim: int = 384,\n","        num_patch_row: int = 2,\n","        image_size: int = 32,\n","        num_blocks: int = 12,\n","        heads: int = 8,\n","        hidden_dim: int = 384 * 4,\n","        mask_ratio: float = 0.75,\n","        dropout: float = 0.0,\n","    ) -> None:\n","        \"\"\"Encoder of Masked AutoEncoder\n","\n","        Parameters\n","        ----------\n","        input_channels : int, optional\n","            Number of input image channels, by default 3\n","        emb_dim : int, optional\n","            Embedding dimensions, by default 384\n","        num_patch_row : int, optional\n","            Number of patch's row, by default 2\n","        image_size : int, optional\n","            Image size. Input images must have the same height and width,\n","            by default 32\n","        num_blocks : int, optional\n","            Number of iterations of transformer blocks, by default 12\n","        heads : int, optional\n","            Number of heads for Multi-Heads Self-Attention, by default 8\n","        hidden_dim : int, optional\n","            Hidden dimensions of Feed Forward Network , by default 384*4\n","        mask_ratio : float, optional\n","            Ratio of masking patches, by default 0.75\n","        dropout : float, optional\n","            Dropout rate, by default 0.0\n","        \"\"\"\n","        super(MAEencoder, self).__init__()\n","\n","        # class token\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n","        # Patch embedding\n","        self.patch_emb = PatchEmbedding(\n","            input_channels, emb_dim, num_patch_row, image_size\n","        )\n","        # Positional embedding\n","        self.pos_emb = nn.Parameter(\n","            torch.randn(1, num_patch_row**2 + 1, emb_dim)\n","        )\n","        # Patch shuffling\n","        self.shuffle = PatchShuffle(mask_ratio)\n","        # Transformer blocks\n","        self.transformer = nn.Sequential(\n","            *[\n","                Block(emb_dim, heads, hidden_dim, dropout)\n","                for _ in range(num_blocks)\n","            ]\n","        )\n","        # Layer Normalization\n","        self.ln = nn.LayerNorm(emb_dim)\n","\n","        # Parameter initialization\n","        self.initialize_weight()\n","\n","    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n","        \"\"\"forward propagation\n","\n","        Parameters\n","        ----------\n","        x : Tensor\n","            input image. shape=(B, C, H, W)\n","            B: batch size, C: number of channel, N: number of patches,\n","            H, W: image size\n","\n","        Returns\n","        -------\n","        features: Tensor\n","            The encoder output, which has the shape (B, N, D).\n","            B: batch size, N: number of patches, D: embedding dimensions\n","        backward_indexes: Tensor\n","            The second element is indexes that restores the patches\n","            to their original order.\n","        \"\"\"\n","        patches = self.patch_emb(x)\n","        patches = torch.cat(\n","            [self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1\n","        )\n","        patches += self.pos_emb\n","        patches, _, backward_indexes = self.shuffle(patches)\n","        features = self.ln(self.transformer(patches))\n","        return features, backward_indexes\n","\n","    def initialize_weight(self) -> None:\n","        nn.init.kaiming_normal_(self.cls_token)\n","        nn.init.kaiming_normal_(self.pos_emb)\n","\n","\n","class MAEdecoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_channels: int = 3,\n","        emb_dim: int = 384,\n","        num_patch_row: int = 2,\n","        image_size: int = 32,\n","        num_blocks: int = 12,\n","        heads: int = 8,\n","        hidden_dim: int = 384 * 4,\n","        dropout: float = 0.0,\n","    ) -> None:\n","        \"\"\"Decoder of Masked AutoEncoder\n","\n","        Parameters\n","        ----------\n","        input_channels : int, optional\n","            Number of input image channels, by default 3\n","        emb_dim : int, optional\n","            Embedding dimensions, by default 384\n","        num_patch_row : int, optional\n","            Number of patch's row, by default 2\n","        image_size : int, optional\n","            Image size. Input images must have the same height and width,\n","            by default 32\n","        num_blocks : int, optional\n","            Number of iterations of transformer blocks, by default 12\n","        heads : int, optional\n","            Number of heads for Multi-Heads Self-Attention, by default 8\n","        hidden_dim : int, optional\n","            Hidden dimensions of Feed Forward Network , by default 384*4\n","        dropout : float, optional\n","            Dropout rate, by default 0.0\n","        \"\"\"\n","        super(MAEdecoder, self).__init__()\n","        self.input_channels = input_channels\n","        self.image_size = image_size\n","\n","        # mask token\n","        self.mask_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n","\n","        # Positional embedding\n","        self.pos_emb = nn.Parameter(\n","            torch.randn(1, num_patch_row**2 + 1, emb_dim)\n","        )\n","        # Transformer blocks\n","        self.transformer = nn.Sequential(\n","            *[\n","                Block(emb_dim, heads, hidden_dim, dropout)\n","                for _ in range(num_blocks)\n","            ]\n","        )\n","\n","        self.reconstruction = nn.Linear(\n","            emb_dim, input_channels * (image_size // num_patch_row) ** 2\n","        )\n","\n","        # Parameter initialization\n","        self.initialize_weight()\n","\n","    def forward(\n","        self, features: Tensor, backward_indexes: Tensor\n","    ) -> tuple[Tensor, Tensor]:\n","        \"\"\"Forward propagation\n","\n","        Parameters\n","        ----------\n","        features: Tensor\n","            The encoder output, which has the shape (B, n+1, D).\n","            B: batch size, n: number of unmasked patches,\n","            D: embedding dimensions\n","        backward_indexes: Tensor\n","            The second element is indexes that restores the patches\n","            to their original order.\n","\n","        Returns\n","        -------\n","        img: Tensor\n","            reconstructed image.\n","        mask: Tensor\n","        \"\"\"\n","        num_patches = features.shape[1]\n","        # (B, n + 1, D) -> (B, N + 1, D)\n","        # N is the number of all patches.\n","        features = torch.cat(\n","            [\n","                features,\n","                self.mask_token.repeat(\n","                    features.shape[0],\n","                    # calculation the number of masked patches\n","                    backward_indexes.shape[1] - features.shape[1],\n","                    1,\n","                ),\n","            ],\n","            dim=1,\n","        )\n","\n","        # Restoring the order of patches\n","        # (B, N + 1, D) -> (B, N + 1, D)\n","        features = PatchShuffle.take_indexes(features, backward_indexes)\n","        features += self.pos_emb\n","\n","        # Decoding with transformer block\n","        # (B, N + 1, D) -> (B, N + 1, D)\n","        features = self.transformer(features)\n","\n","        # Reject cls_token\n","        # (B, N + 1, D) -> (B, N, D)\n","        features = features[:, 1:, :]\n","\n","        # Reconstruction from embedded vector\n","        # (B, N, D) -> (B, N, channels * image_size * image_size)\n","        patches = self.reconstruction(features)\n","\n","        mask = torch.zeros_like(patches)\n","        mask[:, num_patches - 1 :] = 1\n","        mask = PatchShuffle.take_indexes(mask, backward_indexes[:, 1:] - 1)\n","\n","        # Reshape to image\n","        # (B, N, channels * image_size * image_size) --\n","        # --> (B, channels, image_size, image_size)\n","        img = patches.view(\n","            -1, self.input_channels, self.image_size, self.image_size\n","        )\n","        mask = mask.view(\n","            -1, self.input_channels, self.image_size, self.image_size\n","        )\n","\n","        return img, mask\n","\n","    def initialize_weight(self) -> None:\n","        nn.init.kaiming_normal_(self.mask_token)\n","        nn.init.kaiming_normal_(self.pos_emb)\n","\n","\n","class MAEViT(nn.Module):\n","    def __init__(\n","        self,\n","        input_channels: int = 3,\n","        emb_dim: int = 192,\n","        num_patch_row: int = 2,\n","        image_size: int = 32,\n","        encoder_num_blocks: int = 12,\n","        decoder_num_blocks: int = 4,\n","        encoder_heads: int = 3,\n","        decoder_heads: int = 3,\n","        encoder_hidden_dim: int = 768,\n","        decoder_hidden_dim: int = 768,\n","        mask_ratio: float = 0.75,\n","        dropout: float = 0.0,\n","    ) -> None:\n","        \"\"\"_summary_\n","\n","        Parameters\n","        ----------\n","        input_channels : int, optional\n","            Number of input image channels, by default 3\n","        emb_dim : int, optional\n","            Embedding dimensions, by default 384\n","        num_patch_row : int, optional\n","            Number of patch's row, by default 2\n","        image_size : int, optional\n","            Image size. Input images must have the same height and width,\n","            by default 32\n","        num_blocks : int, optional\n","            Number of iterations of transformer blocks, by default 12\n","        heads : int, optional\n","            Number of heads for Multi-Heads Self-Attention, by default 8\n","        hidden_dim : int, optional\n","            Hidden dimensions of Feed Forward Network , by default 384*4\n","        mask_ratio : float, optional\n","            Ratio of masking patches, by default 0.75\n","        dropout : float, optional\n","            Dropout rate, by default 0.0\n","        \"\"\"\n","        super(MAEViT, self).__init__()\n","        self.encoder = MAEencoder(\n","            input_channels,\n","            emb_dim,\n","            num_patch_row,\n","            image_size,\n","            encoder_num_blocks,\n","            encoder_heads,\n","            encoder_hidden_dim,\n","            mask_ratio,\n","            dropout,\n","        )\n","        self.decoder = MAEdecoder(\n","            input_channels,\n","            emb_dim,\n","            num_patch_row,\n","            image_size,\n","            decoder_num_blocks,\n","            decoder_heads,\n","            decoder_hidden_dim,\n","            dropout,\n","        )\n","\n","    def forward(self, img: Tensor) -> tuple[Tensor, Tensor]:\n","        feature, indexes = self.encoder(img)\n","        reconst_img, mask = self.decoder(feature, indexes)\n","        return reconst_img, mask\n","\n","    def get_last_selfattention(self, x: Tensor) -> Tensor:\n","        patches = self.encoder.patch_emb(x)\n","        patches = torch.cat(\n","            [self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches],\n","            dim=1,\n","        )\n","        patches += self.encoder.pos_emb\n","\n","        patches = torch.cat(\n","            [self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches],\n","            dim=1,\n","        )\n","        for i, block in enumerate(self.encoder.transformer):\n","            if i < len(self.encoder.transformer) - 1:\n","                patches = block(patches)\n","            else:\n","                attn: Tensor = block.get_attn(patches)\n","        return attn\n","\n","\n","# cosine scheduler\n","class CosineScheduler:\n","    def __init__(self, epochs, lr, warmup_length=5):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epochs : int\n","            学習のエポック数．\n","        lr : float\n","            学習率．\n","        warmup_length : int\n","            warmupを適用するエポック数．\n","        \"\"\"\n","        self.epochs = epochs\n","        self.lr = lr\n","        self.warmup = warmup_length\n","\n","    def __call__(self, epoch):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epoch : int\n","            現在のエポック数．\n","        \"\"\"\n","        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n","        progress = np.clip(progress, 0.0, 1.0)\n","        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n","\n","        if self.warmup:\n","            lr = lr * min(1., (epoch+1) / self.warmup)\n","\n","        return lr\n","\n","def set_lr(lr, optimizer):\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","# ハイパーパラメータの設定\n","config = {\n","    \"input_channels\": 3,\n","    \"emb_dim\": 192,\n","    \"num_patch_row\": 16,\n","    \"image_size\": 32,\n","    \"encoder_num_blocks\": 12,\n","    \"decoder_num_blocks\": 4,\n","    \"encoder_heads\": 3,\n","    \"decoder_heads\": 3,\n","    \"encoder_hidden_dim\": 768,\n","    \"decoder_hidden_dim\": 768,\n","    \"mask_ratio\": 0.75,\n","    \"dropout\": 0\n","}\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = MAEViT(**config).to(device)\n","\n","epochs = 200\n","lr = 0.003\n","warmup_length = epochs // 10\n","batch_size = 512\n","step_count = 0\n","optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n","scheduler = CosineScheduler(epochs, lr, warmup_length)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1688540880175,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"etMNIZhZ3zal","outputId":"5508994c-bf6d-4470-e4d4-11cbd3b5c619"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["print(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uR8uNlkCxo3d"},"source":["### 事前学習（自己教師あり学習）"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10106797,"status":"ok","timestamp":1688556255439,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"TfIeOGbbxqXx","outputId":"a3cf1ee3-515e-4edb-87fb-558c98294ebb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch[1 / 200] Train Loss: 0.0063 Valid Loss: 0.0066\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m rec_img, mask \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     16\u001b[0m train_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((rec_img \u001b[39m-\u001b[39m x) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m mask) \u001b[39m/\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mmask_ratio\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m step_count \u001b[39m%\u001b[39m \u001b[39m8\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[0;32m/workdir/.venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n","File \u001b[0;32m/workdir/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(epochs):\n","    # スケジューラで学習率を更新する\n","    new_lr = scheduler(epoch)\n","    set_lr(new_lr, optimizer)\n","\n","    total_train_loss = 0.\n","    total_valid_loss = 0.\n","\n","    # モデルの訓練\n","    for x, _ in dataloader_train:\n","        step_count += 1\n","        model.train()\n","        x = x.to(device)\n","\n","        rec_img, mask = model(x)\n","        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n","        train_loss.backward()\n","\n","        if step_count % 8 == 0:  # 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        total_train_loss += train_loss.item()\n","\n","    # モデルの評価\n","    with torch.no_grad():\n","        for x, _ in dataloader_valid:\n","            model.eval()\n","\n","            with torch.no_grad():\n","                x = x.to(device)\n","\n","                rec_img, mask = model(x)\n","                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n","\n","                total_valid_loss += valid_loss.item()\n","\n","\n","    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(dataloader_train):.4f} Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\")\n","\n","# モデルを保存しておく\n","torch.save(model.state_dict(), \"/workdir/tmp/MYmodel/MAE_pretrain_params_v2.pth\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), \"/workdir/tmp/MYmodel/MAE_pretrain_params_v2.pth\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["del model, train_loss, valid_loss\n","torch.cuda.empty_cache()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHOBi4auxuPR"},"source":["### Linear probing"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1688559503295,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"1GeuhPBryfQa"},"outputs":[],"source":["val_size = 3000\n","train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),])# WRITE ME\n","valid_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","test_transform = transforms.Compose([transforms.ToTensor()])# WRITE ME\n","\n","batch_size = 128\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    valid_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_test = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1312,"status":"ok","timestamp":1688559495741,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"_ycgOJ3g-FC3","outputId":"0560003d-ca67-468d-8033-853a20da0c40"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(\"/workdir/tmp/MYmodel/MAE_pretrain_params_v2.pth\"))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":347,"status":"ok","timestamp":1688559508943,"user":{"displayName":"神生龍一","userId":"18016381367682521454"},"user_tz":-540},"id":"z8n5wVT-xvv1"},"outputs":[],"source":["class MLPClassifier(nn.Module):\n","    def __init__(self, hidden_dims: tuple[int, int] = (64, 32), num_classes: int = 10, dropout_ratio: float = 0.3) -> None:\n","        super(MLPClassifier, self).__init__()\n","        self.layer1 = nn.Linear(config[\"emb_dim\"], hidden_dims[0])\n","        self.dropout1=nn.Dropout(dropout_ratio)\n","        self.layer2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n","        self.dropout2 = nn.Dropout(dropout_ratio)\n","        self.layer3 = nn.Linear(hidden_dims[1], num_classes)\n","        \n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.layer1(x)\n","        x = self.dropout1(x)\n","        x = nnf.leaky_relu(x)\n","        x = self.layer2(x)\n","        x = self.dropout2(x)\n","        x = nnf.leaky_relu(x)\n","        x = self.layer3(x)\n","        x = nnf.softmax(x, dim=1)\n","        return x\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","encoder = model.encoder# WRITE ME\n","classifier_model = MLPClassifier().to(device) # WRITE ME\n","epochs = 100\n","lr = 0.001\n","warmup_length = 10\n","batch_size = 64\n","optimizer = optim.AdamW(classifier_model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\n","scheduler = CosineScheduler(epochs, lr, warmup_length)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5T6QiHwyTRj","outputId":"66dbb8f3-d883-4ae0-c6a1-f9c0ecb5339e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch[1 / 100] Train Loss: 2.2816 Train Acc.: 0.1763 Valid Loss: 2.2414 Valid Acc.: 0.2106\n","Epoch[2 / 100] Train Loss: 2.1862 Train Acc.: 0.2824 Valid Loss: 2.1457 Valid Acc.: 0.3194\n","Epoch[3 / 100] Train Loss: 2.1192 Train Acc.: 0.3460 Valid Loss: 2.0942 Valid Acc.: 0.3758\n","Epoch[4 / 100] Train Loss: 2.0823 Train Acc.: 0.3819 Valid Loss: 2.0715 Valid Acc.: 0.3997\n","Epoch[5 / 100] Train Loss: 2.0546 Train Acc.: 0.4090 Valid Loss: 2.0283 Valid Acc.: 0.4409\n","Epoch[6 / 100] Train Loss: 2.0339 Train Acc.: 0.4300 Valid Loss: 2.0218 Valid Acc.: 0.4420\n","Epoch[7 / 100] Train Loss: 2.0138 Train Acc.: 0.4498 Valid Loss: 1.9971 Valid Acc.: 0.4761\n","Epoch[8 / 100] Train Loss: 2.0021 Train Acc.: 0.4607 Valid Loss: 1.9953 Valid Acc.: 0.4693\n","Epoch[9 / 100] Train Loss: 1.9914 Train Acc.: 0.4707 Valid Loss: 1.9699 Valid Acc.: 0.5028\n","Epoch[10 / 100] Train Loss: 1.9848 Train Acc.: 0.4762 Valid Loss: 1.9677 Valid Acc.: 0.4979\n","Epoch[11 / 100] Train Loss: 1.9811 Train Acc.: 0.4796 Valid Loss: 1.9651 Valid Acc.: 0.5006\n","Epoch[12 / 100] Train Loss: 1.9712 Train Acc.: 0.4906 Valid Loss: 1.9652 Valid Acc.: 0.4977\n","Epoch[13 / 100] Train Loss: 1.9665 Train Acc.: 0.4926 Valid Loss: 1.9557 Valid Acc.: 0.5072\n","Epoch[14 / 100] Train Loss: 1.9627 Train Acc.: 0.5000 Valid Loss: 1.9465 Valid Acc.: 0.5175\n","Epoch[15 / 100] Train Loss: 1.9590 Train Acc.: 0.5021 Valid Loss: 1.9499 Valid Acc.: 0.5148\n","Epoch[16 / 100] Train Loss: 1.9558 Train Acc.: 0.5043 Valid Loss: 1.9431 Valid Acc.: 0.5197\n","Epoch[17 / 100] Train Loss: 1.9547 Train Acc.: 0.5067 Valid Loss: 1.9420 Valid Acc.: 0.5228\n","Epoch[18 / 100] Train Loss: 1.9512 Train Acc.: 0.5088 Valid Loss: 1.9358 Valid Acc.: 0.5313\n","Epoch[19 / 100] Train Loss: 1.9464 Train Acc.: 0.5167 Valid Loss: 1.9312 Valid Acc.: 0.5302\n","Epoch[20 / 100] Train Loss: 1.9446 Train Acc.: 0.5153 Valid Loss: 1.9325 Valid Acc.: 0.5283\n","Epoch[21 / 100] Train Loss: 1.9413 Train Acc.: 0.5188 Valid Loss: 1.9254 Valid Acc.: 0.5361\n","Epoch[22 / 100] Train Loss: 1.9396 Train Acc.: 0.5214 Valid Loss: 1.9284 Valid Acc.: 0.5384\n","Epoch[23 / 100] Train Loss: 1.9404 Train Acc.: 0.5214 Valid Loss: 1.9315 Valid Acc.: 0.5323\n","Epoch[24 / 100] Train Loss: 1.9380 Train Acc.: 0.5226 Valid Loss: 1.9191 Valid Acc.: 0.5483\n","Epoch[25 / 100] Train Loss: 1.9353 Train Acc.: 0.5254 Valid Loss: 1.9241 Valid Acc.: 0.5302\n","Epoch[26 / 100] Train Loss: 1.9351 Train Acc.: 0.5266 Valid Loss: 1.9328 Valid Acc.: 0.5262\n","Epoch[27 / 100] Train Loss: 1.9319 Train Acc.: 0.5295 Valid Loss: 1.9286 Valid Acc.: 0.5378\n","Epoch[28 / 100] Train Loss: 1.9286 Train Acc.: 0.5327 Valid Loss: 1.9179 Valid Acc.: 0.5428\n","Epoch[29 / 100] Train Loss: 1.9290 Train Acc.: 0.5344 Valid Loss: 1.9131 Valid Acc.: 0.5510\n","Epoch[30 / 100] Train Loss: 1.9311 Train Acc.: 0.5306 Valid Loss: 1.9141 Valid Acc.: 0.5512\n","Epoch[31 / 100] Train Loss: 1.9281 Train Acc.: 0.5344 Valid Loss: 1.9200 Valid Acc.: 0.5409\n","Epoch[32 / 100] Train Loss: 1.9275 Train Acc.: 0.5345 Valid Loss: 1.9197 Valid Acc.: 0.5391\n","Epoch[33 / 100] Train Loss: 1.9268 Train Acc.: 0.5340 Valid Loss: 1.9170 Valid Acc.: 0.5442\n","Epoch[34 / 100] Train Loss: 1.9232 Train Acc.: 0.5397 Valid Loss: 1.9084 Valid Acc.: 0.5567\n","Epoch[35 / 100] Train Loss: 1.9252 Train Acc.: 0.5369 Valid Loss: 1.9137 Valid Acc.: 0.5483\n","Epoch[36 / 100] Train Loss: 1.9226 Train Acc.: 0.5383 Valid Loss: 1.9106 Valid Acc.: 0.5498\n","Epoch[37 / 100] Train Loss: 1.9245 Train Acc.: 0.5368 Valid Loss: 1.9082 Valid Acc.: 0.5550\n","Epoch[38 / 100] Train Loss: 1.9222 Train Acc.: 0.5387 Valid Loss: 1.9210 Valid Acc.: 0.5414\n","Epoch[39 / 100] Train Loss: 1.9223 Train Acc.: 0.5393 Valid Loss: 1.9155 Valid Acc.: 0.5478\n","Epoch[40 / 100] Train Loss: 1.9221 Train Acc.: 0.5398 Valid Loss: 1.9132 Valid Acc.: 0.5472\n","Epoch[41 / 100] Train Loss: 1.9199 Train Acc.: 0.5426 Valid Loss: 1.9130 Valid Acc.: 0.5483\n","Epoch[42 / 100] Train Loss: 1.9201 Train Acc.: 0.5413 Valid Loss: 1.9105 Valid Acc.: 0.5484\n","Epoch[43 / 100] Train Loss: 1.9166 Train Acc.: 0.5453 Valid Loss: 1.8986 Valid Acc.: 0.5671\n","Epoch[44 / 100] Train Loss: 1.9162 Train Acc.: 0.5441 Valid Loss: 1.9052 Valid Acc.: 0.5598\n","Epoch[45 / 100] Train Loss: 1.9172 Train Acc.: 0.5455 Valid Loss: 1.8997 Valid Acc.: 0.5666\n","Epoch[46 / 100] Train Loss: 1.9174 Train Acc.: 0.5444 Valid Loss: 1.9056 Valid Acc.: 0.5540\n","Epoch[47 / 100] Train Loss: 1.9147 Train Acc.: 0.5469 Valid Loss: 1.8929 Valid Acc.: 0.5711\n","Epoch[48 / 100] Train Loss: 1.9149 Train Acc.: 0.5456 Valid Loss: 1.9049 Valid Acc.: 0.5616\n","Epoch[49 / 100] Train Loss: 1.9116 Train Acc.: 0.5502 Valid Loss: 1.9037 Valid Acc.: 0.5523\n","Epoch[50 / 100] Train Loss: 1.9138 Train Acc.: 0.5488 Valid Loss: 1.9097 Valid Acc.: 0.5514\n","Epoch[51 / 100] Train Loss: 1.9131 Train Acc.: 0.5478 Valid Loss: 1.9076 Valid Acc.: 0.5515\n","Epoch[52 / 100] Train Loss: 1.9109 Train Acc.: 0.5523 Valid Loss: 1.8982 Valid Acc.: 0.5640\n","Epoch[53 / 100] Train Loss: 1.9133 Train Acc.: 0.5479 Valid Loss: 1.9014 Valid Acc.: 0.5611\n","Epoch[54 / 100] Train Loss: 1.9112 Train Acc.: 0.5510 Valid Loss: 1.9019 Valid Acc.: 0.5587\n","Epoch[55 / 100] Train Loss: 1.9106 Train Acc.: 0.5501 Valid Loss: 1.9069 Valid Acc.: 0.5574\n","Epoch[56 / 100] Train Loss: 1.9125 Train Acc.: 0.5482 Valid Loss: 1.9050 Valid Acc.: 0.5593\n","Epoch[57 / 100] Train Loss: 1.9098 Train Acc.: 0.5535 Valid Loss: 1.8960 Valid Acc.: 0.5695\n","Epoch[58 / 100] Train Loss: 1.9081 Train Acc.: 0.5552 Valid Loss: 1.8875 Valid Acc.: 0.5765\n","Epoch[59 / 100] Train Loss: 1.9117 Train Acc.: 0.5514 Valid Loss: 1.9050 Valid Acc.: 0.5539\n","Epoch[60 / 100] Train Loss: 1.9063 Train Acc.: 0.5562 Valid Loss: 1.8964 Valid Acc.: 0.5646\n","Epoch[61 / 100] Train Loss: 1.9087 Train Acc.: 0.5538 Valid Loss: 1.8940 Valid Acc.: 0.5706\n","Epoch[62 / 100] Train Loss: 1.9058 Train Acc.: 0.5570 Valid Loss: 1.8946 Valid Acc.: 0.5587\n","Epoch[63 / 100] Train Loss: 1.9035 Train Acc.: 0.5596 Valid Loss: 1.8902 Valid Acc.: 0.5735\n","Epoch[64 / 100] Train Loss: 1.9057 Train Acc.: 0.5574 Valid Loss: 1.8963 Valid Acc.: 0.5667\n","Epoch[65 / 100] Train Loss: 1.9067 Train Acc.: 0.5540 Valid Loss: 1.8964 Valid Acc.: 0.5660\n","Epoch[66 / 100] Train Loss: 1.9026 Train Acc.: 0.5609 Valid Loss: 1.8966 Valid Acc.: 0.5671\n","Epoch[67 / 100] Train Loss: 1.9062 Train Acc.: 0.5569 Valid Loss: 1.8910 Valid Acc.: 0.5684\n","Epoch[68 / 100] Train Loss: 1.9044 Train Acc.: 0.5577 Valid Loss: 1.8944 Valid Acc.: 0.5733\n","Epoch[69 / 100] Train Loss: 1.9057 Train Acc.: 0.5567 Valid Loss: 1.8922 Valid Acc.: 0.5727\n","Epoch[70 / 100] Train Loss: 1.9034 Train Acc.: 0.5592 Valid Loss: 1.8904 Valid Acc.: 0.5748\n","Epoch[71 / 100] Train Loss: 1.9013 Train Acc.: 0.5623 Valid Loss: 1.8846 Valid Acc.: 0.5786\n","Epoch[72 / 100] Train Loss: 1.9016 Train Acc.: 0.5612 Valid Loss: 1.8934 Valid Acc.: 0.5706\n","Epoch[73 / 100] Train Loss: 1.9032 Train Acc.: 0.5586 Valid Loss: 1.8917 Valid Acc.: 0.5689\n","Epoch[74 / 100] Train Loss: 1.9009 Train Acc.: 0.5624 Valid Loss: 1.9033 Valid Acc.: 0.5566\n","Epoch[75 / 100] Train Loss: 1.9021 Train Acc.: 0.5601 Valid Loss: 1.8859 Valid Acc.: 0.5749\n","Epoch[76 / 100] Train Loss: 1.9017 Train Acc.: 0.5599 Valid Loss: 1.8926 Valid Acc.: 0.5643\n","Epoch[77 / 100] Train Loss: 1.8997 Train Acc.: 0.5634 Valid Loss: 1.8929 Valid Acc.: 0.5659\n","Epoch[78 / 100] Train Loss: 1.9011 Train Acc.: 0.5613 Valid Loss: 1.8873 Valid Acc.: 0.5747\n","Epoch[79 / 100] Train Loss: 1.9010 Train Acc.: 0.5628 Valid Loss: 1.8899 Valid Acc.: 0.5712\n","Epoch[80 / 100] Train Loss: 1.9023 Train Acc.: 0.5607 Valid Loss: 1.8886 Valid Acc.: 0.5787\n","Epoch[81 / 100] Train Loss: 1.8982 Train Acc.: 0.5644 Valid Loss: 1.8937 Valid Acc.: 0.5663\n","Epoch[82 / 100] Train Loss: 1.9005 Train Acc.: 0.5627 Valid Loss: 1.8897 Valid Acc.: 0.5743\n","Epoch[83 / 100] Train Loss: 1.8991 Train Acc.: 0.5644 Valid Loss: 1.8923 Valid Acc.: 0.5683\n","Epoch[84 / 100] Train Loss: 1.8981 Train Acc.: 0.5656 Valid Loss: 1.8888 Valid Acc.: 0.5708\n","Epoch[85 / 100] Train Loss: 1.8988 Train Acc.: 0.5637 Valid Loss: 1.8800 Valid Acc.: 0.5877\n","Epoch[86 / 100] Train Loss: 1.8980 Train Acc.: 0.5642 Valid Loss: 1.8904 Valid Acc.: 0.5774\n","Epoch[87 / 100] Train Loss: 1.8981 Train Acc.: 0.5645 Valid Loss: 1.8833 Valid Acc.: 0.5810\n","Epoch[88 / 100] Train Loss: 1.8973 Train Acc.: 0.5653 Valid Loss: 1.8894 Valid Acc.: 0.5734\n","Epoch[89 / 100] Train Loss: 1.8964 Train Acc.: 0.5670 Valid Loss: 1.8867 Valid Acc.: 0.5784\n","Epoch[90 / 100] Train Loss: 1.8994 Train Acc.: 0.5631 Valid Loss: 1.8950 Valid Acc.: 0.5723\n","Epoch[91 / 100] Train Loss: 1.8974 Train Acc.: 0.5655 Valid Loss: 1.8850 Valid Acc.: 0.5796\n","Epoch[92 / 100] Train Loss: 1.8980 Train Acc.: 0.5648 Valid Loss: 1.8755 Valid Acc.: 0.5928\n","Epoch[93 / 100] Train Loss: 1.8965 Train Acc.: 0.5662 Valid Loss: 1.8878 Valid Acc.: 0.5778\n","Epoch[94 / 100] Train Loss: 1.8967 Train Acc.: 0.5680 Valid Loss: 1.8855 Valid Acc.: 0.5776\n","Epoch[95 / 100] Train Loss: 1.8968 Train Acc.: 0.5681 Valid Loss: 1.8909 Valid Acc.: 0.5706\n","Epoch[96 / 100] Train Loss: 1.8957 Train Acc.: 0.5673 Valid Loss: 1.8920 Valid Acc.: 0.5729\n","Epoch[97 / 100] Train Loss: 1.8966 Train Acc.: 0.5667 Valid Loss: 1.8954 Valid Acc.: 0.5696\n","Epoch[98 / 100] Train Loss: 1.8970 Train Acc.: 0.5652 Valid Loss: 1.8880 Valid Acc.: 0.5778\n","Epoch[99 / 100] Train Loss: 1.8969 Train Acc.: 0.5671 Valid Loss: 1.8875 Valid Acc.: 0.5764\n","Epoch[100 / 100] Train Loss: 1.8949 Train Acc.: 0.5686 Valid Loss: 1.8915 Valid Acc.: 0.5750\n"]}],"source":["encoder.eval()\n","for epoch in range(epochs):\n","    new_lr = scheduler(epoch)\n","    set_lr(new_lr, optimizer)\n","\n","    total_train_loss = 0.\n","    total_train_acc = 0.\n","    total_valid_loss = 0.\n","    total_valid_acc = 0.\n","    for x, t in dataloader_train:\n","        x, t = x.to(device), t.to(device)\n","        feature, _ = encoder(x)\n","        pred = classifier_model(feature[:, 0, :])\n","\n","        train_loss = criterion(pred, t)\n","        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n","\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += train_loss.item()\n","        total_train_acc += train_acc\n","\n","    with torch.no_grad():\n","        for x, t in dataloader_valid:\n","            x, t = x.to(device), t.to(device)\n","            feature, _ = encoder(x)\n","            pred = classifier_model(feature[:, 0, :])\n","\n","            valid_loss = criterion(pred, t)\n","            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n","\n","            total_valid_loss += valid_loss.item()\n","            total_valid_acc += valid_acc\n","\n","    print(f\"Epoch[{epoch+1} / {epochs}]\",\n","          f\"Train Loss: {total_train_loss/len(dataloader_train):.4f}\",\n","          f\"Train Acc.: {total_train_acc/len(dataloader_train):.4f}\",\n","          f\"Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\",\n","          f\"Valid Acc.: {total_valid_acc/len(dataloader_valid):.4f}\")\n","\n","torch.save(classifier_model.state_dict(), \"/workdir/tmp/MYmodel/MAE_classifier_params_v2.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([128, 192])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["feature[:,0,:].shape"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"72n19Q_RyqWl"},"outputs":[],"source":["classifier_model.eval()\n","\n","t_pred = []\n","for x in dataloader_test:\n","    x = x.to(device)\n","    feature, _ = encoder(x)\n","    y = classifier_model(feature[:, 0, :])\n","\n","    # モデルの出力を予測値のスカラーに変換\n","    pred = y.argmax(1).tolist()\n","    t_pred.extend(pred)\n","\n","submission = pd.Series(t_pred, name='label')\n","submission.to_csv(\"submission_pred_v2.csv\", header=True, index_label='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwKJ8IBcy5TG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
